{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import distance\n",
    "\n",
    "import math\n",
    "import pprint\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "from typing import List, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query GraphQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def query_graphql(start_time: int, end_time: int, route: str) -> list:\n",
    "    query = f\"\"\"{{\n",
    "        trynState(agency: \"muni\",\n",
    "                  startTime: \"{start_time}\",\n",
    "                  endTime: \"{end_time}\",\n",
    "                  routes: [\"{route}\"]) {{\n",
    "            agency\n",
    "            startTime\n",
    "            routes {{\n",
    "                stops {{\n",
    "                    sid\n",
    "                    lat\n",
    "                    lon\n",
    "                }}\n",
    "                routeStates {{\n",
    "                    vtime\n",
    "                    vehicles {{\n",
    "                        vid\n",
    "                        lat\n",
    "                        lon\n",
    "                        did\n",
    "                    }}\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    query_url = f\"https://06o8rkohub.execute-api.us-west-2.amazonaws.com/dev/graphql?query={query}\"\n",
    "\n",
    "    request = requests.get(query_url).json()\n",
    "    try:\n",
    "        return request['data']['trynState']['routes']\n",
    "    except KeyError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce Datatables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_stops(data: list, route: str) -> pd.DataFrame:\n",
    "    stops = pd.io.json.json_normalize(data,\n",
    "                                      record_path=['stops']) \\\n",
    "            .rename(columns={'lat': 'LAT',\n",
    "                             'lon': 'LON',\n",
    "                             'sid': 'SID'}) \\\n",
    "            .reindex(['SID', 'LAT', 'LON'], axis='columns')\n",
    "    \n",
    "    # obtain stop directions\n",
    "    stops['DID'] = stops['SID'].map({stop: direction['id']\n",
    "                                     for direction in requests\n",
    "                                                      .get(f\"http://restbus.info/api/agencies/sf-muni/routes/{route}\")\n",
    "                                                      .json()['directions']\n",
    "                                     for stop in direction['stops']})\n",
    "    \n",
    "    # remove stops that don't have an associated direction\n",
    "    stops = stops.dropna(axis='index', subset=['DID'])\n",
    "    \n",
    "    # obtain stop ordinals\n",
    "    stops['ORD'] = stops['SID'].map({stop_meta['id']: ordinal\n",
    "                                     for ordinal, stop_meta\n",
    "                                     in enumerate(requests\n",
    "                                                  .get(\"http://restbus.info/api/agencies/sf-muni/\"\n",
    "                                                       f\"routes/{route}\")\n",
    "                                                  .json()['stops'])})\n",
    "    \n",
    "    return stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def produce_buses(data: list) -> pd.DataFrame:\n",
    "     return pd.io.json.json_normalize(data,\n",
    "                                      record_path=['routeStates', 'vehicles'],\n",
    "                                      meta=[['routeStates', 'vtime']]) \\\n",
    "            .rename(columns={'lat': 'LAT',\n",
    "                             'lon': 'LON',\n",
    "                             'vid': 'VID',\n",
    "                             'did': 'DID',\n",
    "                             'routeStates.vtime': 'TIME'}) \\\n",
    "            .reindex(['TIME', 'VID', 'LAT', 'LON', 'DID'], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eclipses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# haversine formula for calcuating distance between two coordinates in lat lon\n",
    "# from bird eye view; seems to be +- 8 meters difference from geopy distance\n",
    "def haver_distance(latstop,lonstop,latbus,lonbus):\n",
    "\n",
    "    latstop,lonstop,latbus,lonbus = map(np.deg2rad,[latstop,lonstop,latbus,lonbus])\n",
    "    eradius = 6371000\n",
    "    \n",
    "    latdiff = (latbus-latstop)\n",
    "    londiff = (lonbus-lonstop)\n",
    "    \n",
    "    a = np.sin(latdiff/2)**2 + np.cos(latstop)*np.cos(latbus)*np.sin(londiff/2)**2\n",
    "    c = 2*np.arctan2(np.sqrt(a),np.sqrt(1-a))\n",
    "    \n",
    "    distance = eradius*c\n",
    "    return distance\n",
    "\n",
    "def find_eclipses(buses, stop):\n",
    "    \"\"\"\n",
    "    Find movement of buses relative to the stop, in distance as a function of time.\n",
    "    \"\"\"\n",
    "    def split_eclipses(eclipses, threshold=30*60*1000) -> List[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Split buses' movements when they return to a stop after completing the route.\n",
    "        \"\"\"\n",
    "        disjoint_eclipses = []\n",
    "        for bus_id in eclipses['VID'].unique(): # list of unique VID's\n",
    "            # obtain distance data for this one bus\n",
    "            bus = eclipses[eclipses['VID'] == bus_id].sort_values('TIME')\n",
    "            #pprint.pprint(bus)\n",
    "            #pprint.pprint(bus['TIME'].shift())\n",
    "            #pprint.pprint(bus['TIME'].shift() + threshold)\n",
    "            #print('===============')\n",
    "            # split data into groups when there is at least a `threshold`-ms gap between data points\n",
    "            group_ids = (bus['TIME'] > (bus['TIME'].shift() + threshold)).cumsum()\n",
    "\n",
    "            # store groups\n",
    "            for _, group in bus.groupby(group_ids):\n",
    "                disjoint_eclipses.append(group)\n",
    "        return disjoint_eclipses\n",
    "\n",
    "    eclipses = buses.copy()\n",
    "    #eclipses['DIST'] = eclipses.apply(lambda row: distance(stop[['LAT','LON']],row[['LAT','LON']]).meters,axis=1)\n",
    "    \n",
    "    stopcord = stop[['LAT', 'LON']]\n",
    "    buscord = eclipses[['LAT', 'LON']]\n",
    "\n",
    "    # calculate distances fast with haversine function \n",
    "    eclipses['DIST'] = haver_distance(stopcord['LAT'],stopcord['LON'],buscord['LAT'],buscord['LON'])\n",
    "    # only keep positions within 750 meters within the given stop; (filtering out)\n",
    "    eclipses = eclipses[eclipses['DIST'] < 750]\n",
    "    \n",
    "    # update the coordinates list \n",
    "    stopcord = stop[['LAT', 'LON']].values\n",
    "    buscord = eclipses[['LAT', 'LON']].values\n",
    "    \n",
    "    # calculate distances again using geopy for the distance<750m values, because geopy is probably more accurate\n",
    "    dfromstop = []\n",
    "    for row in buscord:\n",
    "        busdistance = distance(stopcord,row).meters\n",
    "        dfromstop.append(busdistance)\n",
    "    eclipses['DIST'] = dfromstop\n",
    "    \n",
    "    # for haversine function:\n",
    "    #stopcord = stop[['LAT', 'LON']]\n",
    "    #buscord = eclipses[['LAT', 'LON']]\n",
    "    #eclipses['DIST'] = haver_distance(stopcord['LAT'],stopcord['LON'],buscord['LAT'],buscord['LON'])\n",
    "    \n",
    "    eclipses['TIME'] = eclipses['TIME'].astype(np.int64)\n",
    "    eclipses = eclipses[['TIME', 'VID', 'DIST']]\n",
    "    \n",
    "    eclipses = split_eclipses(eclipses)\n",
    "    \n",
    "    return eclipses\n",
    "\n",
    "def find_nadirs(eclipses):\n",
    "    \"\"\"\n",
    "    Find points where buses are considered to have encountered the stop.\n",
    "    \n",
    "    Nadir is an astronomical term that describes the lowest point reached by an orbiting body.\n",
    "    \"\"\"\n",
    "    def calc_nadir(eclipse: pd.DataFrame) -> Union[pd.Series, None]:\n",
    "        nadir = eclipse.iloc[eclipse['DIST'].values.argmin()]\n",
    "        if nadir['DIST'] < 100:  # if min dist < 100, then reasonable candidate for nadir\n",
    "            return nadir\n",
    "        else:  # otherwise, hardcore datasci is needed\n",
    "            rev_eclipse = eclipse.iloc[::-1]\n",
    "            rev_nadir = rev_eclipse.iloc[rev_eclipse['DIST'].values.argmin()]\n",
    "            if nadir['TIME'] == rev_nadir['TIME']:  # if eclipse has a global min\n",
    "                return nadir  # then it's the best candidate for nadir\n",
    "            else:  # if eclipse's min occurs at two times\n",
    "                mid_nadir = nadir.copy()\n",
    "                mid_nadir['DIST'] = (nadir['DIST'] + rev_nadir['DIST'])/2\n",
    "                return mid_nadir  # take the midpoint of earliest and latest mins\n",
    "    \n",
    "    nadirs = []\n",
    "    for eclipse in eclipses:\n",
    "        nadirs.append(calc_nadir(eclipse)[['VID', 'TIME']])\n",
    "        \n",
    "    return pd.DataFrame(nadirs)\n",
    "            \n",
    "    \n",
    "def show_stop(eclipses, nadirs):\n",
    "    fig = plt.figure(figsize=(18, 9))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    for eclipse in eclipses:\n",
    "        plt.plot(*eclipse[['TIME', 'DIST']].values.T)\n",
    "        \n",
    "    for nadir_time in nadirs['TIME']:\n",
    "        plt.axvline(nadir_time, linestyle='--', linewidth=.5)\n",
    "\n",
    "    # format plot\n",
    "    ax.get_xaxis().set_major_formatter(  # convert x-axis tick labels to time of day\n",
    "        FuncFormatter(lambda x, p: datetime.fromtimestamp(int(x)//1000).strftime('%I:%M%p')))\n",
    "    plt.title(f\"Eclipses at Stop {stop_id}\"\n",
    "              f\" from {datetime.fromtimestamp(int(start_time)//1000).strftime('%a %b %d %I:%M%p')}\"\n",
    "              f\" to {datetime.fromtimestamp(int(end_time)//1000).strftime('%a %b %d %I:%M%p')}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Distance from Stop (meters)\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardcore Data Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "route = [\"12\", \"14\"]\n",
    "\n",
    "timespan = (\"08:00\",\n",
    "            \"11:00\")\n",
    "\n",
    "dates = [\n",
    "    \"2018-11-12\",\n",
    "    \"2018-11-13\",\n",
    "    \"2018-11-14\",\n",
    "    \"2018-11-15\",\n",
    "    \"2018-11-16\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue: The plot is labeled based on the machine's current timezone, which may not necessarily match the times sent to the API. To remedy this, the logic for displaying the plot would have to be adjusted to account for the UTC offset of the epochs we get back from the API, which I'm hoping there's a module for but I'm not presently familiar with any."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue: currently, if a trajectory looks like `/~V`, the left edge is selected as the nadir. Based on the data, I suspect that the initial upslope may be a GPS glitch as it's being initialized, I believe the trough on the right should be selected instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BusData:\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "    \n",
    "    @property\n",
    "    def routes(self):\n",
    "        return list(self.data.keys())\n",
    "    \n",
    "    @property\n",
    "    def stops(self, route_id):\n",
    "        return list(self.data.get(route_id, {}).keys())\n",
    "    \n",
    "    def append(self, other_data):\n",
    "        for route_id, other_route in other_data.items():\n",
    "            route = self.data.get(route_id)\n",
    "            if route:\n",
    "                for stop_id, other_stop in other_route.items():\n",
    "                    stop = route.get(stop_id)\n",
    "                    if stop:\n",
    "                        stop['eclipses'].extend(other_stop['eclipses'])\n",
    "                    else:\n",
    "                        route[stop_id] = other_stop\n",
    "            else:\n",
    "                self.data[route_id] = other_route\n",
    "    \n",
    "    @classmethod\n",
    "    def read_file(cls, filename):\n",
    "        bus_data = cls()\n",
    "        with open(filename, 'r') as f:\n",
    "            bus_data.append(json.load(f))\n",
    "        return bus_data\n",
    "                \n",
    "    \n",
    "    def write_file(self, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BusData.data` specification:\n",
    "```\n",
    "{\n",
    "    route_id: {  # route_id is a str\n",
    "        stop_id: {  # stop_id is a str\n",
    "            direction_id: str,\n",
    "            order: int,\n",
    "            lat: float,\n",
    "            lon: float,\n",
    "            eclipses: [\n",
    "                {\n",
    "                    bus_id: int,\n",
    "                    timestamp: int,\n",
    "                },\n",
    "                {\n",
    "                    bus_id: int,\n",
    "                    timestamp: int,\n",
    "                },\n",
    "                ...\n",
    "            ]\n",
    "        },\n",
    "        ...\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# grab a couple of sequential stops to look at\n",
    "# get_stop_times(date, stop [time, route, dir])\n",
    "# returns a df w/columns: vID, date, time, stop, route, dir\n",
    "# ISSUE: really slow, any way to speed up graphQL query?\n",
    "\n",
    "# routes = pd.DataFrame(columns = [\"VID\", \"TIME\", \"SID\", \"DID\", \"ROUTE\"])\n",
    "\n",
    "# stop_ids = [stop['id']\n",
    "#             for stop\n",
    "#             in requests.get(f\"http://restbus.info/api/agencies/sf-muni/routes/{route}\").json()['stops']][2:4]\n",
    "\n",
    "# bus_data = BusData()\n",
    "\n",
    "# for stop_id in stop_ids:\n",
    "#     for date in dates:\n",
    "#         start_time = int(datetime.strptime(f\"{date} {timespan[0]} -0800\", \"%Y-%m-%d %H:%M %z\").timestamp())*1000\n",
    "#         end_time   = int(datetime.strptime(f\"{date} {timespan[1]} -0800\", \"%Y-%m-%d %H:%M %z\").timestamp())*1000\n",
    "\n",
    "#         data = query_graphql(start_time, end_time, route)\n",
    "\n",
    "#         if data is None:  # API might refuse to cooperate\n",
    "#             print(\"API probably timed out\")\n",
    "#             continue\n",
    "#         elif len(data) == 0:  # some days somehow have no data\n",
    "#             print(f\"no data for {month}/{day}\")\n",
    "#             continue\n",
    "#         else:\n",
    "#             stops = produce_stops(data)\n",
    "#             buses = produce_buses(data)\n",
    "\n",
    "#             stop = stops[stops['SID'] == stop_id].squeeze()\n",
    "#             buses = buses[buses['DID'] == stop['DID']]\n",
    "\n",
    "#             eclipses = find_eclipses(buses, stop)\n",
    "#             nadirs = find_nadirs(eclipses)\n",
    "#             nadirs[\"TIME\"] = nadirs[\"TIME\"].apply(lambda x: datetime.fromtimestamp(x//1000, timezone(timedelta(hours = -8))).strftime('%a %b %d %I:%M%p'))\n",
    "#             nadirs[\"SID\"] = stop_id\n",
    "#             nadirs[\"DID\"] = stop[\"DID\"]\n",
    "#             nadirs[\"ROUTE\"] = route\n",
    "#             routes = routes.append(nadirs)\n",
    "\n",
    "\n",
    "#             show_stop(eclipses, nadirs)\n",
    "\n",
    "#             bus_data.append({\n",
    "#                 route: {\n",
    "#                     stop_id: {\n",
    "#                         'direction_id': stop['DID'],\n",
    "#                         'order': int(stop['ORD']),\n",
    "#                         'lat': stop['LAT'],\n",
    "#                         'lon': stop['LON'],\n",
    "#                         'eclipses': [\n",
    "#                             {\n",
    "#                                 'bus_id': bus_id,\n",
    "#                                 'timestamp': int(timestamp)\n",
    "#                             }\n",
    "#                             for bus_id, timestamp in zip(nadirs['VID'].tolist(),\n",
    "#                                                          nadirs['TIME'].tolist())\n",
    "#                         ]\n",
    "#                     }\n",
    "#                 }\n",
    "#             })\n",
    "        \n",
    "# bus_data.write_file(\"bus_data.json\")\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_stops\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# parameters:\n",
    "# dates: an array of dates, formatted as strings in the form YYYY-MM-DD\n",
    "# routes: an array of routes, each represented as a string\n",
    "# directions: an array of strings representing the directions to filter\n",
    "# stops: an array of strings representing the stops to filter\n",
    "# times: a tuple with the start and end times (in UTC -8:00) as strings in the form HH:MM \n",
    "# \n",
    "# returns:\n",
    "# stops: a DataFrame, filtered by the given directions and stops, with the following columns:\n",
    "# VID: the vehicle ID\n",
    "# Time: a datetime object representing the date/time of the stop\n",
    "# Route: the route on which the stop occurred\n",
    "# Stop: the stop at which the stop occurred\n",
    "# Dir: the direction in which the stop occurred\n",
    "# -------------------------------------------------------------------------------------------\n",
    "def get_stops(dates, routes, directions = [], new_stops = [], times = (\"00:00\", \"23:59\")):\n",
    "    bus_stops = pd.DataFrame(columns = [\"VID\", \"TIME\", \"SID\", \"DID\", \"ROUTE\"])\n",
    "    \n",
    "    for route in routes:\n",
    "        stop_ids = [stop['id']\n",
    "            for stop\n",
    "            in requests.get(f\"http://restbus.info/api/agencies/sf-muni/routes/{route}\").json()['stops']][2:4]\n",
    "\n",
    "        for stop_id in stop_ids:\n",
    "            # check if stops to filter were provided, or if the stop_id is in the list of filtered stops\n",
    "            if (stop_id in new_stops) ^ (len(new_stops) == 0):\n",
    "                for date in dates:\n",
    "                    print(f\"{datetime.now().strftime('%a %b %d %I:%M:%S %p')}: starting processing on stop {stop_id} on route {route} on {date}.\")\n",
    "                    start_time = int(datetime.strptime(f\"{date} {timespan[0]} -0800\", \"%Y-%m-%d %H:%M %z\").timestamp())*1000\n",
    "                    end_time   = int(datetime.strptime(f\"{date} {timespan[1]} -0800\", \"%Y-%m-%d %H:%M %z\").timestamp())*1000\n",
    "\n",
    "                    data = query_graphql(start_time, end_time, route)\n",
    "                    print(f\"{datetime.now().strftime('%a %b %d %I:%M:%S %p')}: performed query.\")\n",
    "                          \n",
    "                    if data is None:  # API might refuse to cooperate\n",
    "                        print(\"API probably timed out\")\n",
    "                        continue\n",
    "                    elif len(data) == 0:  # some days somehow have no data\n",
    "                        print(f\"no data for {month}/{day}\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        stops = produce_stops(data, route)\n",
    "                        print(f\"{datetime.now().strftime('%a %b %d %I:%M:%S %p')}: produced stops.\")\n",
    "                        #pprint.pprint(stops)\n",
    "                              \n",
    "                        buses = produce_buses(data)\n",
    "                        print(f\"{datetime.now().strftime('%a %b %d %I:%M:%S %p')}: produced buses.\")\n",
    "                        #pprint.pprint(buses)\n",
    "                        \n",
    "                        # select single stop that match stop_id\n",
    "                        stop = stops[stops['SID'] == stop_id].squeeze()\n",
    "                        # select buses that have matching DID with the stop. note* this will not select inbound\n",
    "                        buses = buses[buses['DID'] == stop['DID']]\n",
    "                        \n",
    "                        #pprint.pprint(buses)\n",
    "                        #pprint.pprint(stop)\n",
    "                        \n",
    "                        starttime = time.time()\n",
    "                        eclipses = find_eclipses(buses, stop)\n",
    "                        print('time elapsed for eclipse function: %s' % (time.time() - starttime))\n",
    "                              \n",
    "                        print(f\"{datetime.now().strftime('%a %b %d %I:%M:%S %p')}: found eclipses.\")\n",
    "                              \n",
    "                        nadirs = find_nadirs(eclipses)\n",
    "                        print(f\"{datetime.now().strftime('%a %b %d %I:%M:%S %p')}: found nadirs.\")\n",
    "                            \n",
    "                        nadirs[\"TIME\"] = nadirs[\"TIME\"].apply(lambda x: datetime.fromtimestamp(x//1000, timezone(timedelta(hours = -8))).strftime('%a %b %d %Y %I:%M%p'))\n",
    "                        nadirs[\"SID\"] = stop_id\n",
    "                        nadirs[\"DID\"] = stop[\"DID\"]\n",
    "                        nadirs[\"ROUTE\"] = route\n",
    "                        old_length = len(bus_stops)\n",
    "                        bus_stops = bus_stops.append(nadirs, sort = True)\n",
    "                        print(f\"{datetime.now().strftime('%a %b %d %I:%M:%S %p')}: finished processing.\")\n",
    "\n",
    "    # filter for directions\n",
    "    if len(directions) > 0:\n",
    "        bus_stops = bus_stops.loc[bus_stops['DID'].apply(lambda x: x in directions)]\n",
    "    \n",
    "    return bus_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan 28 02:14:41 PM: starting processing on stop 5528 on route 14 on 2018-11-12.\n",
      "Mon Jan 28 02:14:52 PM: performed query.\n",
      "Mon Jan 28 02:14:53 PM: produced stops.\n",
      "Mon Jan 28 02:14:53 PM: produced buses.\n",
      "before haversine 1548713693.291006\n",
      "after haversine 1548713693.296182\n",
      "before geopy 1548713693.296296\n",
      "time elapsed for eclipse function: 2.8825478553771973\n",
      "Mon Jan 28 02:14:56 PM: found eclipses.\n",
      "Mon Jan 28 02:14:56 PM: found nadirs.\n",
      "Mon Jan 28 02:14:56 PM: finished processing.\n",
      "Mon Jan 28 02:14:56 PM: starting processing on stop 5528 on route 14 on 2018-11-13.\n",
      "Mon Jan 28 02:15:04 PM: performed query.\n",
      "Mon Jan 28 02:15:05 PM: produced stops.\n",
      "Mon Jan 28 02:15:05 PM: produced buses.\n",
      "before haversine 1548713705.874632\n",
      "after haversine 1548713705.879687\n",
      "before geopy 1548713705.879767\n",
      "time elapsed for eclipse function: 2.660742998123169\n",
      "Mon Jan 28 02:15:08 PM: found eclipses.\n",
      "Mon Jan 28 02:15:08 PM: found nadirs.\n",
      "Mon Jan 28 02:15:08 PM: finished processing.\n",
      "Mon Jan 28 02:15:08 PM: starting processing on stop 5528 on route 14 on 2018-11-14.\n",
      "Mon Jan 28 02:15:19 PM: performed query.\n",
      "Mon Jan 28 02:15:19 PM: produced stops.\n",
      "Mon Jan 28 02:15:19 PM: produced buses.\n",
      "before haversine 1548713719.915916\n",
      "after haversine 1548713719.922376\n",
      "before geopy 1548713719.922475\n",
      "time elapsed for eclipse function: 3.0055930614471436\n",
      "Mon Jan 28 02:15:22 PM: found eclipses.\n",
      "Mon Jan 28 02:15:22 PM: found nadirs.\n",
      "Mon Jan 28 02:15:22 PM: finished processing.\n",
      "Mon Jan 28 02:15:22 PM: starting processing on stop 5528 on route 14 on 2018-11-15.\n",
      "Mon Jan 28 02:15:33 PM: performed query.\n",
      "Mon Jan 28 02:15:34 PM: produced stops.\n",
      "Mon Jan 28 02:15:34 PM: produced buses.\n",
      "before haversine 1548713734.043510\n",
      "after haversine 1548713734.048309\n",
      "before geopy 1548713734.048392\n",
      "time elapsed for eclipse function: 3.4749932289123535\n",
      "Mon Jan 28 02:15:37 PM: found eclipses.\n",
      "Mon Jan 28 02:15:37 PM: found nadirs.\n",
      "Mon Jan 28 02:15:37 PM: finished processing.\n",
      "Mon Jan 28 02:15:37 PM: starting processing on stop 5528 on route 14 on 2018-11-16.\n",
      "Mon Jan 28 02:15:47 PM: performed query.\n",
      "Mon Jan 28 02:15:48 PM: produced stops.\n",
      "Mon Jan 28 02:15:48 PM: produced buses.\n",
      "before haversine 1548713748.384353\n",
      "after haversine 1548713748.389583\n",
      "before geopy 1548713748.389684\n",
      "time elapsed for eclipse function: 3.698646068572998\n",
      "Mon Jan 28 02:15:52 PM: found eclipses.\n",
      "Mon Jan 28 02:15:52 PM: found nadirs.\n",
      "Mon Jan 28 02:15:52 PM: finished processing.\n"
     ]
    }
   ],
   "source": [
    "new_stops = get_stops(dates, route, directions = ['14___O_F00'], new_stops = ['5528'], times = timespan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DID</th>\n",
       "      <th>ROUTE</th>\n",
       "      <th>SID</th>\n",
       "      <th>TIME</th>\n",
       "      <th>VID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Mon Nov 12 2018 08:03AM</td>\n",
       "      <td>7225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10499</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Mon Nov 12 2018 10:29AM</td>\n",
       "      <td>7225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Mon Nov 12 2018 08:14AM</td>\n",
       "      <td>7254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10843</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Mon Nov 12 2018 10:34AM</td>\n",
       "      <td>7254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Mon Nov 12 2018 08:21AM</td>\n",
       "      <td>7244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11577</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Mon Nov 12 2018 10:44AM</td>\n",
       "      <td>7244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2107</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Mon Nov 12 2018 08:30AM</td>\n",
       "      <td>7272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12159</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Mon Nov 12 2018 10:53AM</td>\n",
       "      <td>7272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3342</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Mon Nov 12 2018 08:49AM</td>\n",
       "      <td>7232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3658</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Mon Nov 12 2018 08:54AM</td>\n",
       "      <td>7229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2569</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Mon Nov 12 2018 08:37AM</td>\n",
       "      <td>7220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12623</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Mon Nov 12 2018 11:00AM</td>\n",
       "      <td>7220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4266</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Mon Nov 12 2018 09:02AM</td>\n",
       "      <td>7283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4968</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Mon Nov 12 2018 09:12AM</td>\n",
       "      <td>7261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5986</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Mon Nov 12 2018 09:26AM</td>\n",
       "      <td>7250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6060</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Mon Nov 12 2018 09:27AM</td>\n",
       "      <td>7259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6730</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Mon Nov 12 2018 09:36AM</td>\n",
       "      <td>7236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7422</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Mon Nov 12 2018 09:46AM</td>\n",
       "      <td>7204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8540</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Mon Nov 12 2018 10:02AM</td>\n",
       "      <td>7215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8965</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Mon Nov 12 2018 10:08AM</td>\n",
       "      <td>7237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9642</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Mon Nov 12 2018 10:17AM</td>\n",
       "      <td>7275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Tue Nov 13 2018 08:12AM</td>\n",
       "      <td>7220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8907</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Tue Nov 13 2018 10:39AM</td>\n",
       "      <td>7220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Tue Nov 13 2018 08:01AM</td>\n",
       "      <td>7273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8803</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Tue Nov 13 2018 10:22AM</td>\n",
       "      <td>7273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Tue Nov 13 2018 08:17AM</td>\n",
       "      <td>7248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Tue Nov 13 2018 08:23AM</td>\n",
       "      <td>7243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8909</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Tue Nov 13 2018 10:39AM</td>\n",
       "      <td>7243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2256</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Tue Nov 13 2018 08:31AM</td>\n",
       "      <td>7227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9323</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Tue Nov 13 2018 10:53AM</td>\n",
       "      <td>7227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5723</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Thu Nov 15 2018 09:20AM</td>\n",
       "      <td>7264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6266</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Thu Nov 15 2018 09:27AM</td>\n",
       "      <td>7210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6672</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Thu Nov 15 2018 09:33AM</td>\n",
       "      <td>7293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8402</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Thu Nov 15 2018 09:57AM</td>\n",
       "      <td>7234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8966</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Thu Nov 15 2018 10:06AM</td>\n",
       "      <td>7236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9681</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Thu Nov 15 2018 10:16AM</td>\n",
       "      <td>7262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9678</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Thu Nov 15 2018 10:16AM</td>\n",
       "      <td>7222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10737</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Thu Nov 15 2018 10:32AM</td>\n",
       "      <td>7263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Fri Nov 16 2018 08:11AM</td>\n",
       "      <td>7293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Fri Nov 16 2018 08:14AM</td>\n",
       "      <td>7290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11039</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Fri Nov 16 2018 10:35AM</td>\n",
       "      <td>7290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Fri Nov 16 2018 08:02AM</td>\n",
       "      <td>7205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10718</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Fri Nov 16 2018 10:30AM</td>\n",
       "      <td>7205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Fri Nov 16 2018 08:16AM</td>\n",
       "      <td>7229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12063</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Fri Nov 16 2018 10:50AM</td>\n",
       "      <td>7229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3130</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Fri Nov 16 2018 08:45AM</td>\n",
       "      <td>7220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2618</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Fri Nov 16 2018 08:38AM</td>\n",
       "      <td>7234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12732</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Fri Nov 16 2018 11:00AM</td>\n",
       "      <td>7234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4350</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Fri Nov 16 2018 09:02AM</td>\n",
       "      <td>7248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3641</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Fri Nov 16 2018 08:52AM</td>\n",
       "      <td>7206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5137</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Fri Nov 16 2018 09:13AM</td>\n",
       "      <td>7272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5572</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Fri Nov 16 2018 09:19AM</td>\n",
       "      <td>7212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Fri Nov 16 2018 09:25AM</td>\n",
       "      <td>7263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7284</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Fri Nov 16 2018 09:43AM</td>\n",
       "      <td>7232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7441</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Fri Nov 16 2018 09:45AM</td>\n",
       "      <td>7271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7812</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Fri Nov 16 2018 09:50AM</td>\n",
       "      <td>7215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8902</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Fri Nov 16 2018 10:05AM</td>\n",
       "      <td>7211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9343</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Fri Nov 16 2018 10:11AM</td>\n",
       "      <td>7277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9921</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Fri Nov 16 2018 10:19AM</td>\n",
       "      <td>7207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11674</th>\n",
       "      <td>14___O_F00</td>\n",
       "      <td>14</td>\n",
       "      <td>5528</td>\n",
       "      <td>Fri Nov 16 2018 10:44AM</td>\n",
       "      <td>7283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              DID ROUTE   SID                     TIME   VID\n",
       "254    14___O_F00    14  5528  Mon Nov 12 2018 08:03AM  7225\n",
       "10499  14___O_F00    14  5528  Mon Nov 12 2018 10:29AM  7225\n",
       "974    14___O_F00    14  5528  Mon Nov 12 2018 08:14AM  7254\n",
       "10843  14___O_F00    14  5528  Mon Nov 12 2018 10:34AM  7254\n",
       "1460   14___O_F00    14  5528  Mon Nov 12 2018 08:21AM  7244\n",
       "11577  14___O_F00    14  5528  Mon Nov 12 2018 10:44AM  7244\n",
       "2107   14___O_F00    14  5528  Mon Nov 12 2018 08:30AM  7272\n",
       "12159  14___O_F00    14  5528  Mon Nov 12 2018 10:53AM  7272\n",
       "3342   14___O_F00    14  5528  Mon Nov 12 2018 08:49AM  7232\n",
       "3658   14___O_F00    14  5528  Mon Nov 12 2018 08:54AM  7229\n",
       "2569   14___O_F00    14  5528  Mon Nov 12 2018 08:37AM  7220\n",
       "12623  14___O_F00    14  5528  Mon Nov 12 2018 11:00AM  7220\n",
       "4266   14___O_F00    14  5528  Mon Nov 12 2018 09:02AM  7283\n",
       "4968   14___O_F00    14  5528  Mon Nov 12 2018 09:12AM  7261\n",
       "5986   14___O_F00    14  5528  Mon Nov 12 2018 09:26AM  7250\n",
       "6060   14___O_F00    14  5528  Mon Nov 12 2018 09:27AM  7259\n",
       "6730   14___O_F00    14  5528  Mon Nov 12 2018 09:36AM  7236\n",
       "7422   14___O_F00    14  5528  Mon Nov 12 2018 09:46AM  7204\n",
       "8540   14___O_F00    14  5528  Mon Nov 12 2018 10:02AM  7215\n",
       "8965   14___O_F00    14  5528  Mon Nov 12 2018 10:08AM  7237\n",
       "9642   14___O_F00    14  5528  Mon Nov 12 2018 10:17AM  7275\n",
       "882    14___O_F00    14  5528  Tue Nov 13 2018 08:12AM  7220\n",
       "8907   14___O_F00    14  5528  Tue Nov 13 2018 10:39AM  7220\n",
       "123    14___O_F00    14  5528  Tue Nov 13 2018 08:01AM  7273\n",
       "8803   14___O_F00    14  5528  Tue Nov 13 2018 10:22AM  7273\n",
       "1249   14___O_F00    14  5528  Tue Nov 13 2018 08:17AM  7248\n",
       "1649   14___O_F00    14  5528  Tue Nov 13 2018 08:23AM  7243\n",
       "8909   14___O_F00    14  5528  Tue Nov 13 2018 10:39AM  7243\n",
       "2256   14___O_F00    14  5528  Tue Nov 13 2018 08:31AM  7227\n",
       "9323   14___O_F00    14  5528  Tue Nov 13 2018 10:53AM  7227\n",
       "...           ...   ...   ...                      ...   ...\n",
       "5723   14___O_F00    14  5528  Thu Nov 15 2018 09:20AM  7264\n",
       "6266   14___O_F00    14  5528  Thu Nov 15 2018 09:27AM  7210\n",
       "6672   14___O_F00    14  5528  Thu Nov 15 2018 09:33AM  7293\n",
       "8402   14___O_F00    14  5528  Thu Nov 15 2018 09:57AM  7234\n",
       "8966   14___O_F00    14  5528  Thu Nov 15 2018 10:06AM  7236\n",
       "9681   14___O_F00    14  5528  Thu Nov 15 2018 10:16AM  7262\n",
       "9678   14___O_F00    14  5528  Thu Nov 15 2018 10:16AM  7222\n",
       "10737  14___O_F00    14  5528  Thu Nov 15 2018 10:32AM  7263\n",
       "800    14___O_F00    14  5528  Fri Nov 16 2018 08:11AM  7293\n",
       "979    14___O_F00    14  5528  Fri Nov 16 2018 08:14AM  7290\n",
       "11039  14___O_F00    14  5528  Fri Nov 16 2018 10:35AM  7290\n",
       "166    14___O_F00    14  5528  Fri Nov 16 2018 08:02AM  7205\n",
       "10718  14___O_F00    14  5528  Fri Nov 16 2018 10:30AM  7205\n",
       "1141   14___O_F00    14  5528  Fri Nov 16 2018 08:16AM  7229\n",
       "12063  14___O_F00    14  5528  Fri Nov 16 2018 10:50AM  7229\n",
       "3130   14___O_F00    14  5528  Fri Nov 16 2018 08:45AM  7220\n",
       "2618   14___O_F00    14  5528  Fri Nov 16 2018 08:38AM  7234\n",
       "12732  14___O_F00    14  5528  Fri Nov 16 2018 11:00AM  7234\n",
       "4350   14___O_F00    14  5528  Fri Nov 16 2018 09:02AM  7248\n",
       "3641   14___O_F00    14  5528  Fri Nov 16 2018 08:52AM  7206\n",
       "5137   14___O_F00    14  5528  Fri Nov 16 2018 09:13AM  7272\n",
       "5572   14___O_F00    14  5528  Fri Nov 16 2018 09:19AM  7212\n",
       "5997   14___O_F00    14  5528  Fri Nov 16 2018 09:25AM  7263\n",
       "7284   14___O_F00    14  5528  Fri Nov 16 2018 09:43AM  7232\n",
       "7441   14___O_F00    14  5528  Fri Nov 16 2018 09:45AM  7271\n",
       "7812   14___O_F00    14  5528  Fri Nov 16 2018 09:50AM  7215\n",
       "8902   14___O_F00    14  5528  Fri Nov 16 2018 10:05AM  7211\n",
       "9343   14___O_F00    14  5528  Fri Nov 16 2018 10:11AM  7277\n",
       "9921   14___O_F00    14  5528  Fri Nov 16 2018 10:19AM  7207\n",
       "11674  14___O_F00    14  5528  Fri Nov 16 2018 10:44AM  7283\n",
       "\n",
       "[111 rows x 5 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: parse direction as inbound/outbound? (remove route indicator)\n",
    "# filter by direction/stop if provided\n",
    "# split date/time\n",
    "new_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['14___O_F00'], dtype=object)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_stops['DID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['5528'], dtype=object)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_stops['SID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_waiting_time(df, start_time, end_time):\n",
    "    minute_range = [start_time.replace(minute = start_time.minute + i) for i in range(end_time.minute - start_time.minute)]\n",
    "    wait_times = pd.DataFrame(columns = [\"ROUTE\", \"TIME\", \"WAIT\"])\n",
    "    \n",
    "    for minute in minute_range:\n",
    "        print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stops[\"timestamp\"] = new_stops[\"TIME\"].apply(lambda x: datetime.strptime(x, '%a %b %d %Y %I:%M%p').timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stops['date'] = new_stops[\"TIME\"].apply(lambda x: datetime.strptime(x, '%a %b %d %Y %I:%M%p').date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = new_stops[['date', 'timestamp']].pivot_table(values = ['timestamp'], index = ['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-82-f1e297e488d5>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-82-f1e297e488d5>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pivot['timestamp'] = pivot['timestamp'].apply(lambda x: )\u001b[0m\n\u001b[0m                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pivot['timestamp'] = pivot['timestamp'].apply(lambda x: )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
